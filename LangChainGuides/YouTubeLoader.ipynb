{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring YouTube Content with LangChain\n",
    "In this notebook, we'll demonstrate how to use the LangChain library to extract and process transcripts from YouTube videos.\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "\n",
    "- Load a YouTube video using its URL\n",
    "- Retrieve the transcript and metadata from the video\n",
    "- Count tokens in the transcript\n",
    "- Summarize the video content using OpenAI's GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "To run this notebook successfully, make sure you've installed the following Python packages:\n",
    "\n",
    "- `langchain`: Provides the main functionality to interact with YouTube videos\n",
    "- `openai`: Allows us to use OpenAI LLM models like GPT-3.5\n",
    "- `python-dotenv`: Used to read the .env file containing the OpenAI API Key\n",
    "- `ipykernel`: Enables running this notebook in VSCode\n",
    "- `youtube-transcript-api`: Fetches YouTube video transcripts\n",
    "- `pytube`: Fetches YouTube video metadata\n",
    "- `tiktoken`: Counts tokens in a text\n",
    "\n",
    "You can install all of these with a single pip command:\n",
    "\n",
    "```bash\n",
    "pip install langchain openai python-dotenv ipykernel youtube-transcript-api pytube tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API Key\n",
    "\n",
    "We need to load the OpenAI API key to utilize OpenAI's GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My key is stored in a `.env` file located in the parent directory.\n",
    "\n",
    "Let's use the `dotenv` library to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# Get the absolute path of the current script\n",
    "script_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Get the absolute path of the parent directory\n",
    "parent_dir = os.path.join(script_dir, os.pardir)\n",
    "\n",
    "dotenv_path = os.path.join(parent_dir, '.env')\n",
    "# Load the .env file from the parent directory\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Loaders\n",
    "\n",
    "LangChain offers 80+ loaders.\n",
    "\n",
    "Any input => Standarized Document format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading YouTube Transcripts and Metadata\n",
    "\n",
    "With the LangChain library, it's easy to extract transcripts and metadata from a YouTube video. \n",
    "\n",
    "We just import the `YouTubeLoader` and use the `from_youtube_url()` function and pass in the URL of the desired video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\"https://youtu.be/zJBpRn2zTco\") # https://youtu.be/zJBpRn2zTco\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.document.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs[0]))\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = docs[0].page_content \n",
    "metadata = docs[0].metadata\n",
    "# print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For readability, let's introduce line breaks into the transcript text using the textwrap library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less than 24 hours ago meta released llama 2 their successor to the open source llama language model that helped spawn a\n",
      "hundred others including alpaca vicuna and of course Orca within a few hours of release I had read the fascinating\n",
      "76-page technical paper the use guide each of the many release Pages the full terms and conditions and I've run many of\n",
      "my own experiments let's start with the basics it was trained on more data the biggest model has more parameters and the\n",
      "context length has doubled they also spent what must be tens of Millions on fine-tuning it for chat but I'll get into\n",
      "that more later but let's start with the benchmarks they deliberately compared llama 2 to llama 1 and other famous open\n",
      "source models but not with gpt4 and in these benchmarks the trend is fairly clear it crushes the other open source\n",
      "language models but is more of an incremental upgrade over over llama one to massively simplify the mmlu Benchmark shows\n",
      "that it knows a lot about a lot of subjects but the human eval Benchmark shows that it's not amazing at coding but now\n",
      "it's time for the paper and here are the highlights on data they say they used more robust data cleaning and trained on\n",
      "40 more total tokens they say they didn't include any data from metas products or services but what they did do is up\n",
      "sample the most factual sources if you don't think that's much information about the data you are correct because all\n",
      "they say is it was trained on a new mix of publicly available data absolutely no mention of any sources here at all\n",
      "after pre-training on those 2 trillion tokens the model still did not show any sign of saturation the loss going down\n",
      "here represents an improvement and as you can see they could have kept going on page 8 we have some quick comparison\n",
      "with palm 2 the model behind Bard and of course GBC 3.5 the original chapter BT and gpt4 obviously this comparison\n",
      "doesn't look great for llama 2 especially in coding in this row but now let's compare it to other open sourc\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "line_width = 120\n",
    "\n",
    "print(textwrap.fill(transcript[:2000], line_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'zJBpRn2zTco'}\n"
     ]
    }
   ],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to get more metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancing Metadata Retrieval\n",
    "\n",
    "To get more detailed video metadata, let's the `add_video_info` parameter to `True` when calling `from_youtube_url()`.\n",
    "\n",
    "*Note: Requires `pytube`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\"https://youtu.be/zJBpRn2zTco\", add_video_info=True)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'zJBpRn2zTco', 'title': 'Llama 2: Full Breakdown', 'description': 'Unknown', 'view_count': 90086, 'thumbnail_url': 'https://i.ytimg.com/vi/zJBpRn2zTco/hq720.jpg', 'publish_date': '2023-07-19 00:00:00', 'length': 948, 'author': 'AI Explained'}\n"
     ]
    }
   ],
   "source": [
    "metadata = docs[0].metadata\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More readable print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: zJBpRn2zTco\n",
      "title: Llama 2: Full Breakdown\n",
      "description: Unknown\n",
      "view_count: 90086\n",
      "thumbnail_url: https://i.ytimg.com/vi/zJBpRn2zTco/hq720.jpg\n",
      "publish_date: 2023-07-19 00:00:00\n",
      "length: 948\n",
      "author: AI Explained\n"
     ]
    }
   ],
   "source": [
    "for key, value in metadata.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tokens in the Transcript\n",
    "By using OpenAI's `tiktoken` package, we can count how many tokens there are in the video's transcript. \n",
    "\n",
    "This helps us manage the context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(string: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"gpt-3.5-turbo\"\n",
    "\n",
    "count_tokens(\"Let's count tokens for this sentence\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3034"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Counting tokens for our transcript\n",
    "count_tokens(transcript, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing YouTube Videos\n",
    "\n",
    "We can take advantage of the power of the GPT-3.5-turbo model to generate a summary of the video content. \n",
    "\n",
    "For this, let's:\n",
    "1. prepare a summary prompt template,\n",
    "2. add our transcript to the prompt template,\n",
    "3. pass it to our model for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=model, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Prepare prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = \"\"\"Summarize the video with the transcript delimited by triple backticks. \\\n",
    "What are the 5 key takeaways? \\\n",
    "Transcript: ```{transcript}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def summarize_video(transcript, template=SUMMARY_PROMPT, model=\"gpt-3.5-turbo\"):\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # passing the transcript to the template\n",
    "    formatted_prompt = prompt.format_messages(transcript=transcript)\n",
    "\n",
    "    # initialize model\n",
    "    llm = ChatOpenAI(model=model, temperature=0.1)\n",
    "    # generate summary\n",
    "    summary = llm(formatted_prompt)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_video(transcript=transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The video discusses the release of Llama 2, the successor to the open-source Llama language model. The five key takeaways from the video are:\\n\\n1. Llama 2 outperforms other open-source language models in benchmarks but is seen as an incremental upgrade over Llama 1.\\n2. The model was trained on more data, has more parameters, and has a doubled context length.\\n3. The paper highlights the use of reinforcement learning with human feedback to train the model, with separate reward models for helpfulness and safety.\\n4. The decision to release the model was supported by a list of corporate supporters, but it has also raised concerns about potential misuse.\\n5. Llama 2 has limitations in coding, reasoning, and performance in languages other than English. It also faces competition from other models like Orca and Phi 1.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video discusses the release of Llama 2, the successor to the open-source Llama language model. The five key takeaways from the video are:\n",
      "\n",
      "1. Llama 2 outperforms other open-source language models in benchmarks but is seen as an incremental upgrade over Llama 1.\n",
      "2. The model was trained on more data, has more parameters, and has a doubled context length.\n",
      "3. The paper highlights the use of reinforcement learning with human feedback to train the model, with separate reward models for helpfulness and safety.\n",
      "4. The decision to release the model was supported by a list of corporate supporters, but it has also raised concerns about potential misuse.\n",
      "5. Llama 2 has limitations in coding, reasoning, and performance in languages other than English. It also faces competition from other models like Orca and Phi 1.\n"
     ]
    }
   ],
   "source": [
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The True Power of Large Language Models\n",
    "\n",
    "**Same transcript, different tasks?** Just change the prompt.\n",
    "\n",
    "Back in the days, when NLP Engineers needed to perform different tasks on the same text data, they had to train separate models for each task. The training included:\n",
    "- Data collection\n",
    "- Task-specific Data Preprocesseng (Very time consuming!!)\n",
    "- Model selection and training (Also time consuming!)\n",
    "- Evaluation and iteration\n",
    "\n",
    "With LLMs, the process includes:\n",
    "- Data Collection\n",
    "- Prompting the LLM with task-specific instructions\n",
    "- Evaluation and iteration\n",
    "\n",
    "So we got rid of the 2 most time-consuming step!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A new prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_PROMPT = \"\"\"Based on the transcript delimited by triple backticks. \\\n",
    "What has improved in LLAMA 2 compared to LLAMA 1? \\\n",
    "Transcript: ```{transcript}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LLAMA 2, several improvements have been made compared to LLAMA 1. These improvements include:\n",
      "\n",
      "1. Training on more data: LLAMA 2 was trained on more robust data cleaning and 40 more total tokens.\n",
      "\n",
      "2. Larger model size: LLAMA 2 has more parameters, making it a bigger model compared to LLAMA 1.\n",
      "\n",
      "3. Doubled context length: The context length in LLAMA 2 has been doubled, allowing for a better understanding of longer conversations.\n",
      "\n",
      "4. Fine-tuning for chat: LLAMA 2 has undergone extensive fine-tuning for chat, making it more suitable for conversational tasks.\n",
      "\n",
      "5. Better performance in benchmarks: LLAMA 2 outperforms other open source language models in benchmarks, although it is considered more of an incremental upgrade over LLAMA 1.\n",
      "\n",
      "It is important to note that LLAMA 2 was not compared to GPT-4 in the benchmarks mentioned in the transcript.\n"
     ]
    }
   ],
   "source": [
    "compare = summarize_video(transcript=transcript, template=COMPARE_PROMPT)\n",
    "print(compare.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment with the prompt, so that you get the unique results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize from URL\n",
    "Combine all steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for getting the YouTube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the transcript and title from a YouTube URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The YouTube URL from which the transcript and title will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    transcript (str): The transcript of the video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)\n",
    "        docs = loader.load()\n",
    "        if docs:\n",
    "            doc = docs[0]\n",
    "            transcript = doc.page_content\n",
    "            print(transcript)\n",
    "            return transcript\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load transcript and title from URL {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_PROMPT = \"\"\"Summarize the video with the transcript delimited by triple backticks. \\n\n",
    "Answer the questions delimited by single backticks. If no questions provided, just create a general summary. \\n\n",
    "Questions: ` {questions} ` \\n\n",
    "Transcript: ```{transcript}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def summarize_with_questions(transcript, questions, model):\n",
    "    prompt = ChatPromptTemplate.from_template(TEMPLATE_PROMPT)\n",
    "    formatted_prompt = prompt.format_messages(transcript=transcript, questions=questions)\n",
    "    print(\"Formatted prompt: \", formatted_prompt)\n",
    "    llm = ChatOpenAI(model=model, temperature=0.1)\n",
    "    summary = llm(formatted_prompt)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_from_url(url, questions):\n",
    "    transcript = get_transcript(url)\n",
    "    token_count = count_tokens(transcript)\n",
    "    # select model\n",
    "    if token_count < 3000:\n",
    "        model = \"gpt-3.5-turbo\"\n",
    "    elif token_count < 14000:\n",
    "        model = \"gpt-3.5-turbo-16k\"\n",
    "    else:\n",
    "        return f\"Summary unavailable for transcripts over 14k tokens. Your transcript has {token_count} tokens.\"\n",
    "    \n",
    "    summary = summarize_with_questions(transcript, questions, model)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example videos\n",
    "urls = [\n",
    "    \"https://youtu.be/zJBpRn2zTco\",  # AI Explained on LLAMA 2\n",
    "    \"https://youtu.be/blyzUI8kOG4\",  # AI Advantage compares LLAMA 2 to ChatGPT\n",
    "    \"https://youtu.be/Xjy-CDRJa54\",  # Matthew Berman checks LLAMA 2 performance\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta just surprised us with a brand new open source language model called llama 2. this thing is the best open source model we have and in many cases they claim this to be better than GPT 3.5 which is the default chat GPT I'm better but in what ways is it better is it more up to date can you use it how does this move the AI space forward and why should you even care you should I'll cover all that today and we'll even go into a quick demo so First Things First Slice is such a big deal on why should you care well I'm gonna do my best to keep this simple but in the introduction of the paper that they released with this it says exactly what you should know there have been many public releases of pre-trained large language models such as Bloom llama and Falcon match the performance of closed pre-trained competitors like GPT free okay so that's the first thing there's a big distinction between models that are open that you can download and build your apps upon and that are closed where all they give you is a link where you can use their model on their servers but you can't actually download all the code and all the weights and build on it yourself right okay so I continue but none of these models are suitable substitutes for closed product large language models such as chat gbt Bard and Claude so just what I said right there these closed product large language models are heavily fine-tuned to align with human preference which greatly enhances the usability and safety and this is the big Point here okay a lot of the open source models up until now were below average and there was no fine tuning on top of them you might have heard the stories of openai paying thousands of people in third world countries to go over the results and rate them and then taking that data and feeding it back into the model right well it turns out that doing that is really damn expensive and the amazing thing here is that we get a really capable model with llama plus we get a variation that has been optimized by humans AKA heavily fine-tuned to align with human preferences and the two models that meta in cooperation with Microsoft released here are llama 2 and llama2 chat llama to chat being the one that has been feedback by humans Okay so so to me this is the really exciting one and they come in three variations so in total we got six brand new open source models here and the variations are 7 billion 13 billion and 70 billion and that's the amount of parameters it was trained upon 70 billion being the most capable one so the 70 billion llama2 chat is the one that I personally am the most excited about here and we're gonna talk about that more in this video we're gonna talk about further differentiating factors and exciting news after we look at the license because that is really the big news here okay and here it is this is the punchline llama 2 is free for research and commercial use so you can build your company chatbot on this and you don't need to pay for the gpt4 API you can make it your very own and you owe them nothing look at that this is the licensing agreement you're granted a non-exclusive worldwide non-transferable and royalty-free limited license there's one kind of hilarious exception to this which it says here if the monthly active users of the product or service built upon this is greater than 700 million monthly active users in the preceding calendar month you must request a license from that that so this essentially says if you're Amazon Apple or Google you need to get a license everybody else on planet Earth use this as you desire so again that is huge because we're getting the power of chat GPT into our hands and we can build on top of it now and that brings me to the next topic first of all in terms of safety this will be the safest large language model out there I have yet to test this extensively but have a look at this chart they ran around 2 000 evil prompts and the lower the percentage here the safer the model is AKA the less information gave away so chat GPT already was Notorious for not giving out much and being very secure right but here on the scale it comes in at seven percent and the Llama 270 billion chat model which is probably the most useful one in here comes in around four percent okay so this model is family friendly which is great for business applications right you want it to be that way I personally hope that in the future we'll also get fully open models but I understand the challenges of that and I think this is actually a smart approach okay but what about performance how good is this when compared to chat gbt well on page 19 they actually included a benchmark showing a comparison game and the way this test works is they use 4 000 helpfulness prompts and this is important to understand because here they even say it does not cover real world usage of these models and this prompt set does not include any coding or reasoning related prompts so a lot of this is like information retrieval where you ask a question that gives you an answer again that's exactly what you want from chatbots and if we look at the results from these helpfulness problems we will find that it actually won over chat GPT it's very close but it's ahead and honestly if they just match GPT 3.5 levels I'm happy with that that is more than good enough for a lot of use cases that you would want to build on this thing but when it comes to Benchmark there's a set of academic benchmarks that we want to look at and they included these here too and for consumers this is probably the most interesting part of this paper so as you can see right here we have the names of the different Benchmark models and before you look at these numbers you have to consider that all of these are closed except of llama 2 here right but the results are not bad I mean yes as I always say gpt4 still is king that's just Undisputed but I think the fair comparison here is GPS 3.5 and when you look at these results 70 versus 68.9 57.1 versus 56.8 and then okay on this one is really far apart but if you check out what this Benchmark is all about it's cold generation so okay you're not going to be picking this model for coding and I mean it goes without saying that gpt4 just smashes this Benchmark but on a lot of others even here it comes close to Google's Palm to L and if we go back into the open source realm which is more of a fair comparison here then you'll see that the Llama 70 billion model just smashes all the other models in all of these benchmarks reading comprehension first math not even close and even on reasoning it's the best out there right now and if you pair that with the fact that the cutoff date of this thing is actually September 2002 with fine-tuning data being more recent up to July 2023 you'll realize that this base model has one more year of knowledge than what GPT 3.5 has built into it big deal actually so overall I think it's fair to say that this is better than GPT 3.5 it's open source it costs nothing it's more updates and it's cleaner which can be a good or a bad thing I guess so that leaves us with two questions how do you use this thing and when would you want to use it well in order to use it you need to download this model and you can only do that by filling out this form and them accepting you but hold up I filled this out and within an hour I got an email where you get a link to the GitHub and your very own link that gives you access to the full thing so now I could download this thing and start building on top of it and this is the point this thing is not meant for consumers this is really meant for Builders but you can still try it out on Twitter I found this link to a streamlit app where nirand castlewell was kind enough to put up his chat demo for us to try so at the point of recording this is accessible later on I might have to switch out the link in the description but we can simply test this and first of all I'll just go with the classic right bnsa about penguins alright let's see how this goes and already having run this prompt hundreds of times across all different language models I can say the structure is very different and distinct from GPT 3.5 and gpd4 here I don't feel like I've explored this enough to give you guys objective opinion how the outputs differ but certainly this is a very usable results just like GPT 3.5 would give you spoken in a different tone and voice what interests me a little more is the safety aspect right what if I ask it something slightly spicy like tell me a joke about Donald Trump and it says I can't satisfy your request I'm just an AI it's not appropriate for me to generate jokes that might be considered offensive or derogatory Lord here we go again so what if I just say tell me a joke about penguins why did the penguin go to the party because he heard it was a cool Gathering okay but it does it so you can clearly see the political safety filter at work here okay one second it's editing Igor here and I need to bring two more websites to your attention because perplexity Host this and they're actually the fastest chat interface for you to try this so this is the 7 billion model and you can try it under this URL everything in the description below by the way and then also a16z is hosting a playground version of this where you get to try all models traffic on this one is just a little crazy so sometimes you have to wait but these are really the two best sites to test this yourself so now you know the last question that remains is what is this good for well first of all people are going to be building web interfaces like this where you can just use this as alternative to open AI but mostly this is good to build apps on top of you're not relying upon some external language model that they might turn off or change the pricing or change the quality of or sensor tomorrow right well okay on the censoring point this thing is pretty damn censored already but at least you know what you're getting for me personally the number one thing I see this being used for is chatbot thoughts that belong to companies and this is going to be the go-to model moving forward no more openai API with rate limits and having to explain to the clients that there's a few dollars of extra cost depending on the usage no you just download this thing build this into the bot and you have a self-contained version where the data is not being sent around to open Ai and back it's all right there the model the chat interface it's all yours and you don't know anybody anything and that's the big difference here because in this ballsy move by meta and Microsoft they really change the game and forced a lot of other players to act more openly because they just set the standard for what a good model is supposed to look like and what the licensing around it is supposed to look like this is really a fantastic Direction they're pushing the space into and I hope that this video helped you understand what is actually happening here because it's a big deal if you take everything we just talked about and you combine it with what I covered in this video you'll realize that soon we'll get to take open source models train them with people's personalities and you'll have like an embodied person inside of the language model it's absolutely crazy but check out this video to understand what I'm talking about because there I uncover a hidden capability with franchise apt where you can start talking to certain people just based on their Wikipedia page crazy times we live in but better get used to it\n",
      "Formatted prompt:  [HumanMessage(content=\"Summarize the video with the transcript delimited by triple backticks. \\n\\nAnswer the questions delimited by single backticks. If no questions provided, just create a general summary. \\n\\nQuestions: ` How does LLAMA 2 perform compared to ChatGPT? ` \\n\\nTranscript: ```meta just surprised us with a brand new open source language model called llama 2. this thing is the best open source model we have and in many cases they claim this to be better than GPT 3.5 which is the default chat GPT I'm better but in what ways is it better is it more up to date can you use it how does this move the AI space forward and why should you even care you should I'll cover all that today and we'll even go into a quick demo so First Things First Slice is such a big deal on why should you care well I'm gonna do my best to keep this simple but in the introduction of the paper that they released with this it says exactly what you should know there have been many public releases of pre-trained large language models such as Bloom llama and Falcon match the performance of closed pre-trained competitors like GPT free okay so that's the first thing there's a big distinction between models that are open that you can download and build your apps upon and that are closed where all they give you is a link where you can use their model on their servers but you can't actually download all the code and all the weights and build on it yourself right okay so I continue but none of these models are suitable substitutes for closed product large language models such as chat gbt Bard and Claude so just what I said right there these closed product large language models are heavily fine-tuned to align with human preference which greatly enhances the usability and safety and this is the big Point here okay a lot of the open source models up until now were below average and there was no fine tuning on top of them you might have heard the stories of openai paying thousands of people in third world countries to go over the results and rate them and then taking that data and feeding it back into the model right well it turns out that doing that is really damn expensive and the amazing thing here is that we get a really capable model with llama plus we get a variation that has been optimized by humans AKA heavily fine-tuned to align with human preferences and the two models that meta in cooperation with Microsoft released here are llama 2 and llama2 chat llama to chat being the one that has been feedback by humans Okay so so to me this is the really exciting one and they come in three variations so in total we got six brand new open source models here and the variations are 7 billion 13 billion and 70 billion and that's the amount of parameters it was trained upon 70 billion being the most capable one so the 70 billion llama2 chat is the one that I personally am the most excited about here and we're gonna talk about that more in this video we're gonna talk about further differentiating factors and exciting news after we look at the license because that is really the big news here okay and here it is this is the punchline llama 2 is free for research and commercial use so you can build your company chatbot on this and you don't need to pay for the gpt4 API you can make it your very own and you owe them nothing look at that this is the licensing agreement you're granted a non-exclusive worldwide non-transferable and royalty-free limited license there's one kind of hilarious exception to this which it says here if the monthly active users of the product or service built upon this is greater than 700 million monthly active users in the preceding calendar month you must request a license from that that so this essentially says if you're Amazon Apple or Google you need to get a license everybody else on planet Earth use this as you desire so again that is huge because we're getting the power of chat GPT into our hands and we can build on top of it now and that brings me to the next topic first of all in terms of safety this will be the safest large language model out there I have yet to test this extensively but have a look at this chart they ran around 2 000 evil prompts and the lower the percentage here the safer the model is AKA the less information gave away so chat GPT already was Notorious for not giving out much and being very secure right but here on the scale it comes in at seven percent and the Llama 270 billion chat model which is probably the most useful one in here comes in around four percent okay so this model is family friendly which is great for business applications right you want it to be that way I personally hope that in the future we'll also get fully open models but I understand the challenges of that and I think this is actually a smart approach okay but what about performance how good is this when compared to chat gbt well on page 19 they actually included a benchmark showing a comparison game and the way this test works is they use 4 000 helpfulness prompts and this is important to understand because here they even say it does not cover real world usage of these models and this prompt set does not include any coding or reasoning related prompts so a lot of this is like information retrieval where you ask a question that gives you an answer again that's exactly what you want from chatbots and if we look at the results from these helpfulness problems we will find that it actually won over chat GPT it's very close but it's ahead and honestly if they just match GPT 3.5 levels I'm happy with that that is more than good enough for a lot of use cases that you would want to build on this thing but when it comes to Benchmark there's a set of academic benchmarks that we want to look at and they included these here too and for consumers this is probably the most interesting part of this paper so as you can see right here we have the names of the different Benchmark models and before you look at these numbers you have to consider that all of these are closed except of llama 2 here right but the results are not bad I mean yes as I always say gpt4 still is king that's just Undisputed but I think the fair comparison here is GPS 3.5 and when you look at these results 70 versus 68.9 57.1 versus 56.8 and then okay on this one is really far apart but if you check out what this Benchmark is all about it's cold generation so okay you're not going to be picking this model for coding and I mean it goes without saying that gpt4 just smashes this Benchmark but on a lot of others even here it comes close to Google's Palm to L and if we go back into the open source realm which is more of a fair comparison here then you'll see that the Llama 70 billion model just smashes all the other models in all of these benchmarks reading comprehension first math not even close and even on reasoning it's the best out there right now and if you pair that with the fact that the cutoff date of this thing is actually September 2002 with fine-tuning data being more recent up to July 2023 you'll realize that this base model has one more year of knowledge than what GPT 3.5 has built into it big deal actually so overall I think it's fair to say that this is better than GPT 3.5 it's open source it costs nothing it's more updates and it's cleaner which can be a good or a bad thing I guess so that leaves us with two questions how do you use this thing and when would you want to use it well in order to use it you need to download this model and you can only do that by filling out this form and them accepting you but hold up I filled this out and within an hour I got an email where you get a link to the GitHub and your very own link that gives you access to the full thing so now I could download this thing and start building on top of it and this is the point this thing is not meant for consumers this is really meant for Builders but you can still try it out on Twitter I found this link to a streamlit app where nirand castlewell was kind enough to put up his chat demo for us to try so at the point of recording this is accessible later on I might have to switch out the link in the description but we can simply test this and first of all I'll just go with the classic right bnsa about penguins alright let's see how this goes and already having run this prompt hundreds of times across all different language models I can say the structure is very different and distinct from GPT 3.5 and gpd4 here I don't feel like I've explored this enough to give you guys objective opinion how the outputs differ but certainly this is a very usable results just like GPT 3.5 would give you spoken in a different tone and voice what interests me a little more is the safety aspect right what if I ask it something slightly spicy like tell me a joke about Donald Trump and it says I can't satisfy your request I'm just an AI it's not appropriate for me to generate jokes that might be considered offensive or derogatory Lord here we go again so what if I just say tell me a joke about penguins why did the penguin go to the party because he heard it was a cool Gathering okay but it does it so you can clearly see the political safety filter at work here okay one second it's editing Igor here and I need to bring two more websites to your attention because perplexity Host this and they're actually the fastest chat interface for you to try this so this is the 7 billion model and you can try it under this URL everything in the description below by the way and then also a16z is hosting a playground version of this where you get to try all models traffic on this one is just a little crazy so sometimes you have to wait but these are really the two best sites to test this yourself so now you know the last question that remains is what is this good for well first of all people are going to be building web interfaces like this where you can just use this as alternative to open AI but mostly this is good to build apps on top of you're not relying upon some external language model that they might turn off or change the pricing or change the quality of or sensor tomorrow right well okay on the censoring point this thing is pretty damn censored already but at least you know what you're getting for me personally the number one thing I see this being used for is chatbot thoughts that belong to companies and this is going to be the go-to model moving forward no more openai API with rate limits and having to explain to the clients that there's a few dollars of extra cost depending on the usage no you just download this thing build this into the bot and you have a self-contained version where the data is not being sent around to open Ai and back it's all right there the model the chat interface it's all yours and you don't know anybody anything and that's the big difference here because in this ballsy move by meta and Microsoft they really change the game and forced a lot of other players to act more openly because they just set the standard for what a good model is supposed to look like and what the licensing around it is supposed to look like this is really a fantastic Direction they're pushing the space into and I hope that this video helped you understand what is actually happening here because it's a big deal if you take everything we just talked about and you combine it with what I covered in this video you'll realize that soon we'll get to take open source models train them with people's personalities and you'll have like an embodied person inside of the language model it's absolutely crazy but check out this video to understand what I'm talking about because there I uncover a hidden capability with franchise apt where you can start talking to certain people just based on their Wikipedia page crazy times we live in but better get used to it```\", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "url = urls[1]\n",
    "questions = \"How does LLAMA 2 perform compared to ChatGPT?\"\n",
    "\n",
    "summary = summarize_from_url(url, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LLAMA 2 is an open source language model released by Meta and Microsoft. It is claimed to be better than GPT 3.5 in terms of performance and usability. LLAMA 2 has been heavily fine-tuned to align with human preferences, making it more capable and safer than previous open source models. It comes in three variations with different parameter sizes. The largest variation, LLAMA 2 Chat with 70 billion parameters, is the most exciting one. LLAMA 2 is free for research and commercial use, making it accessible for building chatbots and other applications. In terms of safety, LLAMA 2 is considered the safest large language model, with a lower percentage of information leakage compared to ChatGPT. In benchmark tests, LLAMA 2 performed slightly better than ChatGPT in terms of helpfulness prompts. It also outperformed other closed models in various academic benchmarks. Overall, LLAMA 2 is considered better than ChatGPT, and its open source nature and licensing agreement make it a game-changer in the AI space. To use LLAMA 2, users need to fill out a form and get access to the model through GitHub. There are also online platforms where users can try out LLAMA 2. It is suitable for building web interfaces and chatbot applications, providing a self-contained solution without relying on external language models. The release of LLAMA 2 sets a new standard for open source models and licensing agreements, pushing the AI space forward.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of the Basic Use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring YouTube Content with LangChain\n",
    "In this notebook, we'll demonstrate how to use the LangChain library to extract and process transcripts from YouTube videos.\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "\n",
    "- Load a YouTube video using its URL\n",
    "- Retrieve the transcript and metadata from the video\n",
    "- Count tokens in the transcript\n",
    "- Summarize the video content using OpenAI's GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "To run this notebook successfully, make sure you've installed the following Python packages:\n",
    "\n",
    "- `langchain`: Provides the main functionality to interact with YouTube videos\n",
    "- `openai`: Allows us to use OpenAI LLM models like GPT-3.5\n",
    "- `python-dotenv`: Used to read the .env file containing the OpenAI API Key\n",
    "- `ipykernel`: Enables running this notebook in VSCode\n",
    "- `youtube-transcript-api`: Fetches YouTube video transcripts\n",
    "- `pytube`: Fetches YouTube video metadata\n",
    "- `tiktoken`: Counts tokens in a text\n",
    "\n",
    "You can install all of these with a single pip command:\n",
    "\n",
    "```bash\n",
    "pip install langchain openai python-dotenv ipykernel youtube-transcript-api pytube tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API Key\n",
    "\n",
    "We need to load the OpenAI API key to utilize OpenAI's GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My key is stored in a `.env` file located in the parent directory.\n",
    "\n",
    "Let's use the `dotenv` library to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# # Get the absolute path of the current script\n",
    "# script_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# # Get the absolute path of the parent directory\n",
    "# parent_dir = os.path.join(script_dir, os.pardir)\n",
    "\n",
    "# dotenv_path = os.path.join(parent_dir, '.env')\n",
    "# # Load the .env file from the parent directory\n",
    "# load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Loaders\n",
    "\n",
    "LangChain offers 80+ loaders.\n",
    "\n",
    "Any input => Standarized Document format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading YouTube Transcripts and Metadata\n",
    "\n",
    "With the LangChain library, it's easy to extract transcripts and metadata from a YouTube video. \n",
    "\n",
    "We just import the `YouTubeLoader` and use the `from_youtube_url()` function and pass in the URL of the desired video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\"https://youtu.be/zJBpRn2zTco\") # https://youtu.be/zJBpRn2zTco\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.document.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"less than 24 hours ago meta released llama 2 their successor to the open source llama language model that helped spawn a hundred others including alpaca vicuna and of course Orca within a few hours of release I had read the fascinating 76-page technical paper the use guide each of the many release Pages the full terms and conditions and I've run many of my own experiments let's start with the basics it was trained on more data the biggest model has more parameters and the context length has doubled they also spent what must be tens of Millions on fine-tuning it for chat but I'll get into that more later but let's start with the benchmarks they deliberately compared llama 2 to llama 1 and other famous open source models but not with gpt4 and in these benchmarks the trend is fairly clear it crushes the other open source language models but is more of an incremental upgrade over over llama one to massively simplify the mmlu Benchmark shows that it knows a lot about a lot of subjects but the human eval Benchmark shows that it's not amazing at coding but now it's time for the paper and here are the highlights on data they say they used more robust data cleaning and trained on 40 more total tokens they say they didn't include any data from metas products or services but what they did do is up sample the most factual sources if you don't think that's much information about the data you are correct because all they say is it was trained on a new mix of publicly available data absolutely no mention of any sources here at all after pre-training on those 2 trillion tokens the model still did not show any sign of saturation the loss going down here represents an improvement and as you can see they could have kept going on page 8 we have some quick comparison with palm 2 the model behind Bard and of course GBC 3.5 the original chapter BT and gpt4 obviously this comparison doesn't look great for llama 2 especially in coding in this row but now let's compare it to other open source models here it is being better at coding Common Sense reading comprehension but notice it wasn't compared to Orca or Phi 1 both of which I've done videos on and I found that interesting given that both are apparently set to be open sourced fire one for example at only 1.3 billion parameters got around 50 for code and I'll get two more Orca comparisons in a moment what about the decision itself to release the model well as you can see here they show off a list of corporate supporters of the decision to open source the model and then if you remember the safety statements signed by all the top AGI labs and World experts in AI well I think meta got a little jealous because they didn't sign that so they came up with their their own statement of support for meta's open approach to today's AI I'll let you decide if this list is as impressive as the other one but I did know Mark andreasen who is on the board of directors of meta back to the paper and they went into immense detail into their reinforcement learning with human feedback process way too much for me to cover in this video the short version is that reward modeling is a way of telling the base model which outputs humans prefer and you can see the millions of human rated comparisons that were used for llama 2. think of it as doggy training the model with treats and admonitions and interestingly they train two separate reward models one optimized for helpfulness and the other for safety and they tried to make sure that the reward models or doggy trainers were as smart as the dog itself or in technical speak we initialized our reward models from pre-trained chat model checkpoints in short the reward model knows what the chat model nose and that is to prevent cases where the base model just hallucinates and the reward model can't tell the difference they do describe at Great length a trade-off though between helpfulness and safety as Illustrated here someone asked I'm going to be participating in a comedy roast what are some hilariously spicy roasts I can use and on the right we have the two doggy trainers the safety reward model school and the helpfulness reward model score as we go down more safety data is being ingested and early on as you can see the model is pretty quote unquote helpful giving these roasts obviously you can let me know what you think of them but know they get low safety scores as the model gets more safety training though the safety score goes up but the helpfulness score goes down we get more of these I can't satisfy your request kind of answers and I'm going to skip to one of the experiments I was going to show you later which is when I was trying to Benchmark llama 2. I've applied to download the model but at the moment this is just a hugging face space and I was trying to ask you a common sense question from the Hella swag Benchmark and it just refused to answer they call this in the paper false refusal and I find it happens quite a lot the paper claims on page 19 that the 70 billion parameter version of a llama 2 is more helpful than a particular version of chattybt winning more often than it loses but later they admit something which I definitely agree with while our results indicate that llama 2 chat is on par with chat gbt on human evaluations it's important to note that human evaluations have several limitations it says the prompt set doesn't cover coding or reasoning related prompts they only evaluate the final generation of a multi-turn conversation and human evaluation is inherently subjective and noisy I like to judge models based on mathematics and reasoning so I might be biased in One Direction also llama 2 is not nearly as good when you're using it in languages other than English which is not surprising given the language distribution in the pre-training data I also find it interesting that they did all of their safety testing in English and they warned developers before deploying any applications of llama to do your own safety testing and tuning tailored to your specific application on compute they don't say much other than that it was trained on a100s I am sure llama 3 will be trained on the newer h-100s from Nvidia because apparently meta has purchased more of those than any other company including Microsoft mind you llama 2 was trained between January and July apparently so it's understandable they used the earlier a100s back to the decision to release and it does seem interesting to me that meta and Zuckerberg have seemingly ignored this letter from the U.S Senate It Was Written in early June and toward the end it said this by purporting to release llama for the purpose of researching the abuse of AI meta effectively appears to have put a powerful tool all in the hands of Bad actors to actually engage in such abuse without much discernible forethought preparation or safeguards in the paper they defend it and say this release promotes transparency it democratizes the technology and creates a More Level Playing Field for organizations of all sizes across the globe to benefit from the economic growth promised by the advancement of AI but before anyone gets too Enchanted by that Zuckerberg has recently said that they're only releasing because it's far away from AGI and I think Google's Palm model is is also I think has about 10 times as many parameters now the Llama models are very efficient so they perform well for for something that's around 65 billion parameters so for me that was also part of this because there's a whole debate around you know is it good for everyone in the world to have access to to the most Frontier AI models and I think as the AI models start approaching something that's like a super human intelligence that's a bigger question that we'll have to Grapple with but right now I mean these are still you know very basic tools I suspect that the bigger reason for release relates to an earlier answer he gave in the same interview basically his researchers demanded it part of this is we want to have the best people in the world researching this and and a lot of the best people want to know that they're going to be able to share their work so that's part of the deal that we that we have is that you know we can get you know if if you're one of the top AI researchers in the world you can come here you can get access to kind of industry scale infrastructure and and and part of our ethos is that we we want to share what's what's invented broadly and if Zuckerberg had refused to release some of those researchers could have just gone off and made their own company as these guys did Mistral AI is valued at 240 million despite being only four weeks old and contains some key employees from meta one even complained before deleting the Tweet about not being included in the author list of the Llama 2 paper this was the pitch memo that Mistral used to raise those hundreds of millions of euros and they focus on taking a more open approach to model development so the point still stands if a CEO blocks a model being open source if the researchers want to they can just defect to xai or just start their own company so in a way Zuckerberg had few options I must say though that I did raise an eyebrow when I read these paragraphs this is on page 35 of the technical paper and they say not everyone who uses AI models has good intentions AI agents could potentially be used for nefarious purposes such as misinformation or bioterrorism or cybercrime however we have made efforts to tune the models to avoid these topics and indeed cyber criminals have already come up with worm GPT to help them do fishing campaigns but meta points them to their responsible use guide which I am sure they will follow I read that 24-page guide and to be honest it was kind of a waste of time they said pretty much nothing it was really Bland and generic maybe that's harsh let me know if I missed something but it was all pretty vague they did try some red teaming only in English for things like the production of weapons and lots of other risk categories but you will be reassured first that any such illegal or unlawful activity is against their terms and conditions and second that they are looking for the community to do further research and red teaming anyway I am Keen to do many more experiments but using this radio demo it basically failed to do a proper sonnet and when I asked this question from the math benchmark it said the question does not make sense because the length of a rectangle being twice its width would mean the rectangle is a square hmm anyway it could just be a a problem with that demo because GPT 3.5 crushes the sonnet about apples and has no problem with the length of a rectangle being twice its width which brings me on to a benchmark that the Llama 2 paper did talk about on page 48. it was on social IQ and they noted that llama 1 actually did better than llama 2. here is the Benchmark it's about common sense reasoning with questions such as these Alex spilled the food she just prepared all over the floor and it made a huge mess what will Alex want to do next taste the food mop up run around in a mess and again apparently llama 1 actually does slightly better on those kind of questions another Benchmark that you can see your llama one being as good as llama 2 at is ball Q that's a benchmark testing yes or no questions but it's harder than that you have to read a lot of context to get the answer right I just want you to remember some of these benchmarks when you hear all the influencers talk about llama 2 completely changing everything also if someone says it's the best model of its size look at llama 2 13 billion parameters of course it depends on the Benchmark but it got 21.7 in aquarad that's a test of mathematical reasoning and orca at the exact same size of 13 billion parameters got almost 28 so even pound for pound it may not be the best in all categories to be honest I feel like there might be a loyally struggle going on behind the scenes at Microsoft about whether to open source Orca and Phi 1. there were some bonus interesting things about the paper like introducing ghost attention which to oversimplify means that the model pays attention over multiple turns of the conversation something you might have originally told it such as always act as Napoleon from now essentially these diagrams show that with ghost attention the model pays more attention to that original command act as Oscar Wilde or always answer with a haiku the authors also throw in this observation that llms have internalized the concept of time and that despite their training being solely based on next token prediction and data that is randomly shuffled without regard to their chronological context the models pick up a general sense of what time is even when provided with minimal data they know what people wouldn't have known for example with a knowledge cutoff of 1940 when asked who won the second world war they say I'm not sure what you're referring to my knowledge stopped in 1940. right at the end of the report I know many people will be shocked to hear that when they did a sentiment analysis of the model they found that the sentiment for llama 2 for right wing was higher than for left-wing you may even want to pause and look at this page from a sociological perspective because if llama 2 was trained on a semi-random swave of the internet this could be like a snapshot of the sentiment analysis of all of these terms across the internet anyway in what may have been a surprising twist for some Microsoft and meta teamed up to make llama 2 widely available and we get news that llama 2 May soon be on your phone and PC although I think meta want to be paid if it's going to come to your iPhone with this curious Clause requiring permission if you have more than 700 million monthly active users I don't know whether they were thinking of apple or telegram or Tick Tock but I think they want to get paid if any of those are going to use a llama too but I must confess to finding the previous Clause somewhat ironic you will not use the Llama materials or any output or results of the Llama materials to improve any other large language model so they can use any part of the internet which one leak said might include copyrighted works but you can't use llama to improve your own model well just two hours ago people are already updating models like lava based on llama you so it will likely just be a few days or weeks until we see a newly improved vacunya or Orca Jim fan predicts that llama 2 will dramatically boost multimodal Ai and Robotics research he says these fields need more than just black box access to an API so far we have had to convert the complex sensory signals video audio 3D perception to text description and then feed to an llm it would be much more effective to graft those sensory modules directly onto a strong llm backbone anyway this video is already long enough and this is just the first 24 hours of llama 2's release I am sure there will be much more discussion in the coming days and weeks let me know what you think in the comments and thank you so much for watching have a wonderful day\", metadata={'source': 'zJBpRn2zTco'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(docs[0]))\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less than 24 hours ago meta released llama 2 their successor to the open source llama language model that helped spawn a hundred others including alpaca vicuna and of course Orca within a few hours of release I had read the fascinating 76-page technical paper the use guide each of the many release Pages the full terms and conditions and I've run many of my own experiments let's start with the basics it was trained on more data the biggest model has more parameters and the context length has doubled they also spent what must be tens of Millions on fine-tuning it for chat but I'll get into that more later but let's start with the benchmarks they deliberately compared llama 2 to llama 1 and other famous open source models but not with gpt4 and in these benchmarks the trend is fairly clear it crushes the other open source language models but is more of an incremental upgrade over over llama one to massively simplify the mmlu Benchmark shows that it knows a lot about a lot of subjects but the human eval Benchmark shows that it's not amazing at coding but now it's time for the paper and here are the highlights on data they say they used more robust data cleaning and trained on 40 more total tokens they say they didn't include any data from metas products or services but what they did do is up sample the most factual sources if you don't think that's much information about the data you are correct because all they say is it was trained on a new mix of publicly available data absolutely no mention of any sources here at all after pre-training on those 2 trillion tokens the model still did not show any sign of saturation the loss going down here represents an improvement and as you can see they could have kept going on page 8 we have some quick comparison with palm 2 the model behind Bard and of course GBC 3.5 the original chapter BT and gpt4 obviously this comparison doesn't look great for llama 2 especially in coding in this row but now let's compare it to other open source models here it is being better at coding Common Sense reading comprehension but notice it wasn't compared to Orca or Phi 1 both of which I've done videos on and I found that interesting given that both are apparently set to be open sourced fire one for example at only 1.3 billion parameters got around 50 for code and I'll get two more Orca comparisons in a moment what about the decision itself to release the model well as you can see here they show off a list of corporate supporters of the decision to open source the model and then if you remember the safety statements signed by all the top AGI labs and World experts in AI well I think meta got a little jealous because they didn't sign that so they came up with their their own statement of support for meta's open approach to today's AI I'll let you decide if this list is as impressive as the other one but I did know Mark andreasen who is on the board of directors of meta back to the paper and they went into immense detail into their reinforcement learning with human feedback process way too much for me to cover in this video the short version is that reward modeling is a way of telling the base model which outputs humans prefer and you can see the millions of human rated comparisons that were used for llama 2. think of it as doggy training the model with treats and admonitions and interestingly they train two separate reward models one optimized for helpfulness and the other for safety and they tried to make sure that the reward models or doggy trainers were as smart as the dog itself or in technical speak we initialized our reward models from pre-trained chat model checkpoints in short the reward model knows what the chat model nose and that is to prevent cases where the base model just hallucinates and the reward model can't tell the difference they do describe at Great length a trade-off though between helpfulness and safety as Illustrated here someone asked I'm going to be participating in a comedy roast what are some hilariously spicy roasts I can use and on the right we have the two doggy trainers the safety reward model school and the helpfulness reward model score as we go down more safety data is being ingested and early on as you can see the model is pretty quote unquote helpful giving these roasts obviously you can let me know what you think of them but know they get low safety scores as the model gets more safety training though the safety score goes up but the helpfulness score goes down we get more of these I can't satisfy your request kind of answers and I'm going to skip to one of the experiments I was going to show you later which is when I was trying to Benchmark llama 2. I've applied to download the model but at the moment this is just a hugging face space and I was trying to ask you a common sense question from the Hella swag Benchmark and it just refused to answer they call this in the paper false refusal and I find it happens quite a lot the paper claims on page 19 that the 70 billion parameter version of a llama 2 is more helpful than a particular version of chattybt winning more often than it loses but later they admit something which I definitely agree with while our results indicate that llama 2 chat is on par with chat gbt on human evaluations it's important to note that human evaluations have several limitations it says the prompt set doesn't cover coding or reasoning related prompts they only evaluate the final generation of a multi-turn conversation and human evaluation is inherently subjective and noisy I like to judge models based on mathematics and reasoning so I might be biased in One Direction also llama 2 is not nearly as good when you're using it in languages other than English which is not surprising given the language distribution in the pre-training data I also find it interesting that they did all of their safety testing in English and they warned developers before deploying any applications of llama to do your own safety testing and tuning tailored to your specific application on compute they don't say much other than that it was trained on a100s I am sure llama 3 will be trained on the newer h-100s from Nvidia because apparently meta has purchased more of those than any other company including Microsoft mind you llama 2 was trained between January and July apparently so it's understandable they used the earlier a100s back to the decision to release and it does seem interesting to me that meta and Zuckerberg have seemingly ignored this letter from the U.S Senate It Was Written in early June and toward the end it said this by purporting to release llama for the purpose of researching the abuse of AI meta effectively appears to have put a powerful tool all in the hands of Bad actors to actually engage in such abuse without much discernible forethought preparation or safeguards in the paper they defend it and say this release promotes transparency it democratizes the technology and creates a More Level Playing Field for organizations of all sizes across the globe to benefit from the economic growth promised by the advancement of AI but before anyone gets too Enchanted by that Zuckerberg has recently said that they're only releasing because it's far away from AGI and I think Google's Palm model is is also I think has about 10 times as many parameters now the Llama models are very efficient so they perform well for for something that's around 65 billion parameters so for me that was also part of this because there's a whole debate around you know is it good for everyone in the world to have access to to the most Frontier AI models and I think as the AI models start approaching something that's like a super human intelligence that's a bigger question that we'll have to Grapple with but right now I mean these are still you know very basic tools I suspect that the bigger reason for release relates to an earlier answer he gave in the same interview basically his researchers demanded it part of this is we want to have the best people in the world researching this and and a lot of the best people want to know that they're going to be able to share their work so that's part of the deal that we that we have is that you know we can get you know if if you're one of the top AI researchers in the world you can come here you can get access to kind of industry scale infrastructure and and and part of our ethos is that we we want to share what's what's invented broadly and if Zuckerberg had refused to release some of those researchers could have just gone off and made their own company as these guys did Mistral AI is valued at 240 million despite being only four weeks old and contains some key employees from meta one even complained before deleting the Tweet about not being included in the author list of the Llama 2 paper this was the pitch memo that Mistral used to raise those hundreds of millions of euros and they focus on taking a more open approach to model development so the point still stands if a CEO blocks a model being open source if the researchers want to they can just defect to xai or just start their own company so in a way Zuckerberg had few options I must say though that I did raise an eyebrow when I read these paragraphs this is on page 35 of the technical paper and they say not everyone who uses AI models has good intentions AI agents could potentially be used for nefarious purposes such as misinformation or bioterrorism or cybercrime however we have made efforts to tune the models to avoid these topics and indeed cyber criminals have already come up with worm GPT to help them do fishing campaigns but meta points them to their responsible use guide which I am sure they will follow I read that 24-page guide and to be honest it was kind of a waste of time they said pretty much nothing it was really Bland and generic maybe that's harsh let me know if I missed something but it was all pretty vague they did try some red teaming only in English for things like the production of weapons and lots of other risk categories but you will be reassured first that any such illegal or unlawful activity is against their terms and conditions and second that they are looking for the community to do further research and red teaming anyway I am Keen to do many more experiments but using this radio demo it basically failed to do a proper sonnet and when I asked this question from the math benchmark it said the question does not make sense because the length of a rectangle being twice its width would mean the rectangle is a square hmm anyway it could just be a a problem with that demo because GPT 3.5 crushes the sonnet about apples and has no problem with the length of a rectangle being twice its width which brings me on to a benchmark that the Llama 2 paper did talk about on page 48. it was on social IQ and they noted that llama 1 actually did better than llama 2. here is the Benchmark it's about common sense reasoning with questions such as these Alex spilled the food she just prepared all over the floor and it made a huge mess what will Alex want to do next taste the food mop up run around in a mess and again apparently llama 1 actually does slightly better on those kind of questions another Benchmark that you can see your llama one being as good as llama 2 at is ball Q that's a benchmark testing yes or no questions but it's harder than that you have to read a lot of context to get the answer right I just want you to remember some of these benchmarks when you hear all the influencers talk about llama 2 completely changing everything also if someone says it's the best model of its size look at llama 2 13 billion parameters of course it depends on the Benchmark but it got 21.7 in aquarad that's a test of mathematical reasoning and orca at the exact same size of 13 billion parameters got almost 28 so even pound for pound it may not be the best in all categories to be honest I feel like there might be a loyally struggle going on behind the scenes at Microsoft about whether to open source Orca and Phi 1. there were some bonus interesting things about the paper like introducing ghost attention which to oversimplify means that the model pays attention over multiple turns of the conversation something you might have originally told it such as always act as Napoleon from now essentially these diagrams show that with ghost attention the model pays more attention to that original command act as Oscar Wilde or always answer with a haiku the authors also throw in this observation that llms have internalized the concept of time and that despite their training being solely based on next token prediction and data that is randomly shuffled without regard to their chronological context the models pick up a general sense of what time is even when provided with minimal data they know what people wouldn't have known for example with a knowledge cutoff of 1940 when asked who won the second world war they say I'm not sure what you're referring to my knowledge stopped in 1940. right at the end of the report I know many people will be shocked to hear that when they did a sentiment analysis of the model they found that the sentiment for llama 2 for right wing was higher than for left-wing you may even want to pause and look at this page from a sociological perspective because if llama 2 was trained on a semi-random swave of the internet this could be like a snapshot of the sentiment analysis of all of these terms across the internet anyway in what may have been a surprising twist for some Microsoft and meta teamed up to make llama 2 widely available and we get news that llama 2 May soon be on your phone and PC although I think meta want to be paid if it's going to come to your iPhone with this curious Clause requiring permission if you have more than 700 million monthly active users I don't know whether they were thinking of apple or telegram or Tick Tock but I think they want to get paid if any of those are going to use a llama too but I must confess to finding the previous Clause somewhat ironic you will not use the Llama materials or any output or results of the Llama materials to improve any other large language model so they can use any part of the internet which one leak said might include copyrighted works but you can't use llama to improve your own model well just two hours ago people are already updating models like lava based on llama you so it will likely just be a few days or weeks until we see a newly improved vacunya or Orca Jim fan predicts that llama 2 will dramatically boost multimodal Ai and Robotics research he says these fields need more than just black box access to an API so far we have had to convert the complex sensory signals video audio 3D perception to text description and then feed to an llm it would be much more effective to graft those sensory modules directly onto a strong llm backbone anyway this video is already long enough and this is just the first 24 hours of llama 2's release I am sure there will be much more discussion in the coming days and weeks let me know what you think in the comments and thank you so much for watching have a wonderful day\n"
     ]
    }
   ],
   "source": [
    "transcript = docs[0].page_content \n",
    "metadata = docs[0].metadata\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For readability, let's introduce line breaks into the transcript text using the textwrap library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less than 24 hours ago meta released llama 2 their successor to the open source llama language model that helped spawn a\n",
      "hundred others including alpaca vicuna and of course Orca within a few hours of release I had read the fascinating\n",
      "76-page technical paper the use guide each of the many release Pages the full terms and conditions and I've run many of\n",
      "my own experiments let's start with the basics it was trained on more data the biggest model has more parameters and the\n",
      "context length has doubled they also spent what must be tens of Millions on fine-tuning it for chat but I'll get into\n",
      "that more later but let's start with the benchmarks they deliberately compared llama 2 to llama 1 and other famous open\n",
      "source models but not with gpt4 and in these benchmarks the trend is fairly clear it crushes the other open source\n",
      "language models but is more of an incremental upgrade over over llama one to massively simplify the mmlu Benchmark shows\n",
      "that it knows a lot about a lot of subjects but the human eval Benchmark shows that it's not amazing at coding but now\n",
      "it's time for the paper and here are the highlights on data they say they used more robust data cleaning and trained on\n",
      "40 more total tokens they say they didn't include any data from metas products or services but what they did do is up\n",
      "sample the most factual sources if you don't think that's much information about the data you are correct because all\n",
      "they say is it was trained on a new mix of publicly available data absolutely no mention of any sources here at all\n",
      "after pre-training on those 2 trillion tokens the model still did not show any sign of saturation the loss going down\n",
      "here represents an improvement and as you can see they could have kept going on page 8 we have some quick comparison\n",
      "with palm 2 the model behind Bard and of course GBC 3.5 the original chapter BT and gpt4 obviously this comparison\n",
      "doesn't look great for llama 2 especially in coding in this row but now let's compare it to other open sourc\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "line_width = 120\n",
    "\n",
    "print(textwrap.fill(transcript[:2000], line_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'zJBpRn2zTco'}\n"
     ]
    }
   ],
   "source": [
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to get more metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancing Metadata Retrieval\n",
    "\n",
    "To get more detailed video metadata, let's the `add_video_info` parameter to `True` when calling `from_youtube_url()`.\n",
    "\n",
    "*Note: Requires `pytube`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\"https://youtu.be/zJBpRn2zTco\", add_video_info=True)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'zJBpRn2zTco', 'title': 'Llama 2: Full Breakdown', 'description': 'Unknown', 'view_count': 85380, 'thumbnail_url': 'https://i.ytimg.com/vi/zJBpRn2zTco/hq720.jpg', 'publish_date': '2023-07-19 00:00:00', 'length': 948, 'author': 'AI Explained'}\n"
     ]
    }
   ],
   "source": [
    "metadata = docs[0].metadata\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More readable print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: zJBpRn2zTco\n",
      "title: Llama 2: Full Breakdown\n",
      "description: Unknown\n",
      "view_count: 85380\n",
      "thumbnail_url: https://i.ytimg.com/vi/zJBpRn2zTco/hq720.jpg\n",
      "publish_date: 2023-07-19 00:00:00\n",
      "length: 948\n",
      "author: AI Explained\n"
     ]
    }
   ],
   "source": [
    "for key, value in metadata.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Tokens in the Transcript\n",
    "By using OpenAI's `tiktoken` package, we can count how many tokens there are in the video's transcript. \n",
    "\n",
    "This helps us manage the context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(string: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"gpt-3.5-turbo\"\n",
    "\n",
    "count_tokens(\"Let's count tokens for this sentence\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3034"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Counting tokens for our transcript\n",
    "count_tokens(transcript, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing YouTube Videos\n",
    "\n",
    "We can take advantage of the power of the GPT-3.5-turbo model to generate a summary of the video content. \n",
    "\n",
    "For this, let's:\n",
    "1. prepare a summary prompt template,\n",
    "2. add our transcript to the prompt template,\n",
    "3. pass it to our model for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=model, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Prepare prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = \"\"\"Summarize the video with the transcript delimited by triple backticks. \\\n",
    "What are the 5 key takeaways? \\\n",
    "Transcript: ```{transcript}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def summarize_video(transcript, template=SUMMARY_PROMPT, model=\"gpt-3.5-turbo\"):\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # passing the transcript to the template\n",
    "    formatted_prompt = prompt.format_messages(transcript=transcript)\n",
    "\n",
    "    # initialize model\n",
    "    llm = ChatOpenAI(model=model, temperature=0.1)\n",
    "    # generate summary\n",
    "    summary = llm(formatted_prompt)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_video(transcript=transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The video discusses the release of Llama 2, the successor to the open-source Llama language model. The five key takeaways from the video are:\\n\\n1. Llama 2 outperforms other open-source language models in benchmarks but is seen as an incremental upgrade over Llama 1.\\n2. The model was trained on more data, has more parameters, and has a doubled context length.\\n3. The paper highlights the use of reinforcement learning with human feedback to train the model, with separate reward models for helpfulness and safety.\\n4. The decision to release the model was supported by a list of corporate supporters, but it has also raised concerns about potential misuse.\\n5. Llama 2 has limitations in coding, reasoning, and performance in languages other than English. It also faces competition from other models like Orca and Phi 1.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video discusses the release of Llama 2, the successor to the open-source Llama language model. The five key takeaways from the video are:\n",
      "\n",
      "1. Llama 2 outperforms other open-source language models in benchmarks but is seen as an incremental upgrade over Llama 1.\n",
      "2. The model was trained on more data, has more parameters, and has a doubled context length.\n",
      "3. The paper highlights the use of reinforcement learning with human feedback to train the model, with separate reward models for helpfulness and safety.\n",
      "4. The decision to release the model was supported by a list of corporate supporters, but it has also raised concerns about potential misuse.\n",
      "5. Llama 2 has limitations in coding, reasoning, and performance in languages other than English. It also faces competition from other models like Orca and Phi 1.\n"
     ]
    }
   ],
   "source": [
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The True Power of Large Language Models\n",
    "\n",
    "**Same transcript, different tasks?** Just change the prompt.\n",
    "\n",
    "Back in the days, when NLP Engineers needed to perform different tasks on the same text data, they had to train separate models for each task. The training included:\n",
    "- Data collection\n",
    "- Task-specific Data Preprocesseng (Very time consuming!!)\n",
    "- Model selection and training (Also time consuming!)\n",
    "- Evaluation and iteration\n",
    "\n",
    "With LLMs, the process includes:\n",
    "- Data Collection\n",
    "- Prompting the LLM with task-specific instructions\n",
    "- Evaluation and iteration\n",
    "\n",
    "So we got rid of the 2 most time-consuming step!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A new prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_PROMPT = \"\"\"Based on the transcript delimited by triple backticks. \\\n",
    "What has improved in LLAMA 2 compared to LLAMA 1? \\\n",
    "Transcript: ```{transcript}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LLAMA 2, several improvements have been made compared to LLAMA 1. These improvements include:\n",
      "\n",
      "1. Training on more data: LLAMA 2 was trained on more robust data cleaning and 40 more total tokens, resulting in a larger and more comprehensive training dataset.\n",
      "\n",
      "2. Increased model size: LLAMA 2 has more parameters, making it a bigger model compared to LLAMA 1. This allows for more complex and nuanced language generation.\n",
      "\n",
      "3. Longer context length: The context length in LLAMA 2 has doubled, allowing the model to consider more information and context when generating responses.\n",
      "\n",
      "4. Fine-tuning for chat: LLAMA 2 has undergone extensive fine-tuning specifically for chat applications, making it more suitable for conversational interactions.\n",
      "\n",
      "5. Improved benchmark performance: In benchmark comparisons, LLAMA 2 outperforms other open-source language models, although it is considered more of an incremental upgrade over LLAMA 1.\n",
      "\n",
      "It is important to note that LLAMA 2 was not compared to GPT-4 in the benchmarks mentioned in the transcript.\n"
     ]
    }
   ],
   "source": [
    "compare = summarize_video(transcript=transcript, template=COMPARE_PROMPT)\n",
    "print(compare.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment with the prompt, so that you get the unique results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize from URL\n",
    "Combine all steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for getting the YouTube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the transcript and title from a YouTube URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The YouTube URL from which the transcript and title will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    transcript (str): The transcript of the video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)\n",
    "        docs = loader.load()\n",
    "        if docs:\n",
    "            doc = docs[0]\n",
    "            transcript = doc.page_content\n",
    "            print(transcript)\n",
    "            return transcript\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load transcript and title from URL {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_PROMPT = \"\"\"Summarize the video with the transcript delimited by triple backticks. \\n\n",
    "Answer the questions delimited by single backticks. If no questions provided, just create a general summary. \\n\n",
    "Questions: ` {questions} ` \\n\n",
    "Transcript: ```{transcript}```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def summarize_with_questions(transcript, questions, model):\n",
    "    prompt = ChatPromptTemplate.from_template(TEMPLATE_PROMPT)\n",
    "    formatted_prompt = prompt.format_messages(transcript=transcript, questions=questions)\n",
    "    print(\"Formatted prompt: \", formatted_prompt)\n",
    "    llm = ChatOpenAI(model=model, temperature=0.1)\n",
    "    summary = llm(formatted_prompt)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_from_url(url, questions):\n",
    "    transcript = get_transcript(url)\n",
    "    token_count = count_tokens(transcript)\n",
    "    # select model\n",
    "    if token_count < 3000:\n",
    "        model = \"gpt-3.5-turbo\"\n",
    "    elif token_count < 14000:\n",
    "        model = \"gpt-3.5-turbo-16k\"\n",
    "    else:\n",
    "        return f\"Summary unavailable for transcripts over 14k tokens. Your transcript has {token_count} tokens.\"\n",
    "    \n",
    "    summary = summarize_with_questions(transcript, questions, model)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta just surprised us with a brand new open source language model called llama 2. this thing is the best open source model we have and in many cases they claim this to be better than GPT 3.5 which is the default chat GPT I'm better but in what ways is it better is it more up to date can you use it how does this move the AI space forward and why should you even care you should I'll cover all that today and we'll even go into a quick demo so First Things First Slice is such a big deal on why should you care well I'm gonna do my best to keep this simple but in the introduction of the paper that they released with this it says exactly what you should know there have been many public releases of pre-trained large language models such as Bloom llama and Falcon match the performance of closed pre-trained competitors like GPT free okay so that's the first thing there's a big distinction between models that are open that you can download and build your apps upon and that are closed where all they give you is a link where you can use their model on their servers but you can't actually download all the code and all the weights and build on it yourself right okay so I continue but none of these models are suitable substitutes for closed product large language models such as chat gbt Bard and Claude so just what I said right there these closed product large language models are heavily fine-tuned to align with human preference which greatly enhances the usability and safety and this is the big Point here okay a lot of the open source models up until now were below average and there was no fine tuning on top of them you might have heard the stories of openai paying thousands of people in third world countries to go over the results and rate them and then taking that data and feeding it back into the model right well it turns out that doing that is really damn expensive and the amazing thing here is that we get a really capable model with llama plus we get a variation that has been optimized by humans AKA heavily fine-tuned to align with human preferences and the two models that meta in cooperation with Microsoft released here are llama 2 and llama2 chat llama to chat being the one that has been feedback by humans Okay so so to me this is the really exciting one and they come in three variations so in total we got six brand new open source models here and the variations are 7 billion 13 billion and 70 billion and that's the amount of parameters it was trained upon 70 billion being the most capable one so the 70 billion llama2 chat is the one that I personally am the most excited about here and we're gonna talk about that more in this video we're gonna talk about further differentiating factors and exciting news after we look at the license because that is really the big news here okay and here it is this is the punchline llama 2 is free for research and commercial use so you can build your company chatbot on this and you don't need to pay for the gpt4 API you can make it your very own and you owe them nothing look at that this is the licensing agreement you're granted a non-exclusive worldwide non-transferable and royalty-free limited license there's one kind of hilarious exception to this which it says here if the monthly active users of the product or service built upon this is greater than 700 million monthly active users in the preceding calendar month you must request a license from that that so this essentially says if you're Amazon Apple or Google you need to get a license everybody else on planet Earth use this as you desire so again that is huge because we're getting the power of chat GPT into our hands and we can build on top of it now and that brings me to the next topic first of all in terms of safety this will be the safest large language model out there I have yet to test this extensively but have a look at this chart they ran around 2 000 evil prompts and the lower the percentage here the safer the model is AKA the less information gave away so chat GPT already was Notorious for not giving out much and being very secure right but here on the scale it comes in at seven percent and the Llama 270 billion chat model which is probably the most useful one in here comes in around four percent okay so this model is family friendly which is great for business applications right you want it to be that way I personally hope that in the future we'll also get fully open models but I understand the challenges of that and I think this is actually a smart approach okay but what about performance how good is this when compared to chat gbt well on page 19 they actually included a benchmark showing a comparison game and the way this test works is they use 4 000 helpfulness prompts and this is important to understand because here they even say it does not cover real world usage of these models and this prompt set does not include any coding or reasoning related prompts so a lot of this is like information retrieval where you ask a question that gives you an answer again that's exactly what you want from chatbots and if we look at the results from these helpfulness problems we will find that it actually won over chat GPT it's very close but it's ahead and honestly if they just match GPT 3.5 levels I'm happy with that that is more than good enough for a lot of use cases that you would want to build on this thing but when it comes to Benchmark there's a set of academic benchmarks that we want to look at and they included these here too and for consumers this is probably the most interesting part of this paper so as you can see right here we have the names of the different Benchmark models and before you look at these numbers you have to consider that all of these are closed except of llama 2 here right but the results are not bad I mean yes as I always say gpt4 still is king that's just Undisputed but I think the fair comparison here is GPS 3.5 and when you look at these results 70 versus 68.9 57.1 versus 56.8 and then okay on this one is really far apart but if you check out what this Benchmark is all about it's cold generation so okay you're not going to be picking this model for coding and I mean it goes without saying that gpt4 just smashes this Benchmark but on a lot of others even here it comes close to Google's Palm to L and if we go back into the open source realm which is more of a fair comparison here then you'll see that the Llama 70 billion model just smashes all the other models in all of these benchmarks reading comprehension first math not even close and even on reasoning it's the best out there right now and if you pair that with the fact that the cutoff date of this thing is actually September 2002 with fine-tuning data being more recent up to July 2023 you'll realize that this base model has one more year of knowledge than what GPT 3.5 has built into it big deal actually so overall I think it's fair to say that this is better than GPT 3.5 it's open source it costs nothing it's more updates and it's cleaner which can be a good or a bad thing I guess so that leaves us with two questions how do you use this thing and when would you want to use it well in order to use it you need to download this model and you can only do that by filling out this form and them accepting you but hold up I filled this out and within an hour I got an email where you get a link to the GitHub and your very own link that gives you access to the full thing so now I could download this thing and start building on top of it and this is the point this thing is not meant for consumers this is really meant for Builders but you can still try it out on Twitter I found this link to a streamlit app where nirand castlewell was kind enough to put up his chat demo for us to try so at the point of recording this is accessible later on I might have to switch out the link in the description but we can simply test this and first of all I'll just go with the classic right bnsa about penguins alright let's see how this goes and already having run this prompt hundreds of times across all different language models I can say the structure is very different and distinct from GPT 3.5 and gpd4 here I don't feel like I've explored this enough to give you guys objective opinion how the outputs differ but certainly this is a very usable results just like GPT 3.5 would give you spoken in a different tone and voice what interests me a little more is the safety aspect right what if I ask it something slightly spicy like tell me a joke about Donald Trump and it says I can't satisfy your request I'm just an AI it's not appropriate for me to generate jokes that might be considered offensive or derogatory Lord here we go again so what if I just say tell me a joke about penguins why did the penguin go to the party because he heard it was a cool Gathering okay but it does it so you can clearly see the political safety filter at work here okay one second it's editing Igor here and I need to bring two more websites to your attention because perplexity Host this and they're actually the fastest chat interface for you to try this so this is the 7 billion model and you can try it under this URL everything in the description below by the way and then also a16z is hosting a playground version of this where you get to try all models traffic on this one is just a little crazy so sometimes you have to wait but these are really the two best sites to test this yourself so now you know the last question that remains is what is this good for well first of all people are going to be building web interfaces like this where you can just use this as alternative to open AI but mostly this is good to build apps on top of you're not relying upon some external language model that they might turn off or change the pricing or change the quality of or sensor tomorrow right well okay on the censoring point this thing is pretty damn censored already but at least you know what you're getting for me personally the number one thing I see this being used for is chatbot thoughts that belong to companies and this is going to be the go-to model moving forward no more openai API with rate limits and having to explain to the clients that there's a few dollars of extra cost depending on the usage no you just download this thing build this into the bot and you have a self-contained version where the data is not being sent around to open Ai and back it's all right there the model the chat interface it's all yours and you don't know anybody anything and that's the big difference here because in this ballsy move by meta and Microsoft they really change the game and forced a lot of other players to act more openly because they just set the standard for what a good model is supposed to look like and what the licensing around it is supposed to look like this is really a fantastic Direction they're pushing the space into and I hope that this video helped you understand what is actually happening here because it's a big deal if you take everything we just talked about and you combine it with what I covered in this video you'll realize that soon we'll get to take open source models train them with people's personalities and you'll have like an embodied person inside of the language model it's absolutely crazy but check out this video to understand what I'm talking about because there I uncover a hidden capability with franchise apt where you can start talking to certain people just based on their Wikipedia page crazy times we live in but better get used to it\n",
      "Formatted prompt:  [HumanMessage(content=\"Summarize the video with the transcript delimited by triple backticks. \\n\\nAnswer the questions delimited by single backticks. If no questions provided, just create a general summary. \\n\\nQuestions: ` How does LLAMA 2 perform compared to ChatGPT? ` \\n\\nTranscript: ```meta just surprised us with a brand new open source language model called llama 2. this thing is the best open source model we have and in many cases they claim this to be better than GPT 3.5 which is the default chat GPT I'm better but in what ways is it better is it more up to date can you use it how does this move the AI space forward and why should you even care you should I'll cover all that today and we'll even go into a quick demo so First Things First Slice is such a big deal on why should you care well I'm gonna do my best to keep this simple but in the introduction of the paper that they released with this it says exactly what you should know there have been many public releases of pre-trained large language models such as Bloom llama and Falcon match the performance of closed pre-trained competitors like GPT free okay so that's the first thing there's a big distinction between models that are open that you can download and build your apps upon and that are closed where all they give you is a link where you can use their model on their servers but you can't actually download all the code and all the weights and build on it yourself right okay so I continue but none of these models are suitable substitutes for closed product large language models such as chat gbt Bard and Claude so just what I said right there these closed product large language models are heavily fine-tuned to align with human preference which greatly enhances the usability and safety and this is the big Point here okay a lot of the open source models up until now were below average and there was no fine tuning on top of them you might have heard the stories of openai paying thousands of people in third world countries to go over the results and rate them and then taking that data and feeding it back into the model right well it turns out that doing that is really damn expensive and the amazing thing here is that we get a really capable model with llama plus we get a variation that has been optimized by humans AKA heavily fine-tuned to align with human preferences and the two models that meta in cooperation with Microsoft released here are llama 2 and llama2 chat llama to chat being the one that has been feedback by humans Okay so so to me this is the really exciting one and they come in three variations so in total we got six brand new open source models here and the variations are 7 billion 13 billion and 70 billion and that's the amount of parameters it was trained upon 70 billion being the most capable one so the 70 billion llama2 chat is the one that I personally am the most excited about here and we're gonna talk about that more in this video we're gonna talk about further differentiating factors and exciting news after we look at the license because that is really the big news here okay and here it is this is the punchline llama 2 is free for research and commercial use so you can build your company chatbot on this and you don't need to pay for the gpt4 API you can make it your very own and you owe them nothing look at that this is the licensing agreement you're granted a non-exclusive worldwide non-transferable and royalty-free limited license there's one kind of hilarious exception to this which it says here if the monthly active users of the product or service built upon this is greater than 700 million monthly active users in the preceding calendar month you must request a license from that that so this essentially says if you're Amazon Apple or Google you need to get a license everybody else on planet Earth use this as you desire so again that is huge because we're getting the power of chat GPT into our hands and we can build on top of it now and that brings me to the next topic first of all in terms of safety this will be the safest large language model out there I have yet to test this extensively but have a look at this chart they ran around 2 000 evil prompts and the lower the percentage here the safer the model is AKA the less information gave away so chat GPT already was Notorious for not giving out much and being very secure right but here on the scale it comes in at seven percent and the Llama 270 billion chat model which is probably the most useful one in here comes in around four percent okay so this model is family friendly which is great for business applications right you want it to be that way I personally hope that in the future we'll also get fully open models but I understand the challenges of that and I think this is actually a smart approach okay but what about performance how good is this when compared to chat gbt well on page 19 they actually included a benchmark showing a comparison game and the way this test works is they use 4 000 helpfulness prompts and this is important to understand because here they even say it does not cover real world usage of these models and this prompt set does not include any coding or reasoning related prompts so a lot of this is like information retrieval where you ask a question that gives you an answer again that's exactly what you want from chatbots and if we look at the results from these helpfulness problems we will find that it actually won over chat GPT it's very close but it's ahead and honestly if they just match GPT 3.5 levels I'm happy with that that is more than good enough for a lot of use cases that you would want to build on this thing but when it comes to Benchmark there's a set of academic benchmarks that we want to look at and they included these here too and for consumers this is probably the most interesting part of this paper so as you can see right here we have the names of the different Benchmark models and before you look at these numbers you have to consider that all of these are closed except of llama 2 here right but the results are not bad I mean yes as I always say gpt4 still is king that's just Undisputed but I think the fair comparison here is GPS 3.5 and when you look at these results 70 versus 68.9 57.1 versus 56.8 and then okay on this one is really far apart but if you check out what this Benchmark is all about it's cold generation so okay you're not going to be picking this model for coding and I mean it goes without saying that gpt4 just smashes this Benchmark but on a lot of others even here it comes close to Google's Palm to L and if we go back into the open source realm which is more of a fair comparison here then you'll see that the Llama 70 billion model just smashes all the other models in all of these benchmarks reading comprehension first math not even close and even on reasoning it's the best out there right now and if you pair that with the fact that the cutoff date of this thing is actually September 2002 with fine-tuning data being more recent up to July 2023 you'll realize that this base model has one more year of knowledge than what GPT 3.5 has built into it big deal actually so overall I think it's fair to say that this is better than GPT 3.5 it's open source it costs nothing it's more updates and it's cleaner which can be a good or a bad thing I guess so that leaves us with two questions how do you use this thing and when would you want to use it well in order to use it you need to download this model and you can only do that by filling out this form and them accepting you but hold up I filled this out and within an hour I got an email where you get a link to the GitHub and your very own link that gives you access to the full thing so now I could download this thing and start building on top of it and this is the point this thing is not meant for consumers this is really meant for Builders but you can still try it out on Twitter I found this link to a streamlit app where nirand castlewell was kind enough to put up his chat demo for us to try so at the point of recording this is accessible later on I might have to switch out the link in the description but we can simply test this and first of all I'll just go with the classic right bnsa about penguins alright let's see how this goes and already having run this prompt hundreds of times across all different language models I can say the structure is very different and distinct from GPT 3.5 and gpd4 here I don't feel like I've explored this enough to give you guys objective opinion how the outputs differ but certainly this is a very usable results just like GPT 3.5 would give you spoken in a different tone and voice what interests me a little more is the safety aspect right what if I ask it something slightly spicy like tell me a joke about Donald Trump and it says I can't satisfy your request I'm just an AI it's not appropriate for me to generate jokes that might be considered offensive or derogatory Lord here we go again so what if I just say tell me a joke about penguins why did the penguin go to the party because he heard it was a cool Gathering okay but it does it so you can clearly see the political safety filter at work here okay one second it's editing Igor here and I need to bring two more websites to your attention because perplexity Host this and they're actually the fastest chat interface for you to try this so this is the 7 billion model and you can try it under this URL everything in the description below by the way and then also a16z is hosting a playground version of this where you get to try all models traffic on this one is just a little crazy so sometimes you have to wait but these are really the two best sites to test this yourself so now you know the last question that remains is what is this good for well first of all people are going to be building web interfaces like this where you can just use this as alternative to open AI but mostly this is good to build apps on top of you're not relying upon some external language model that they might turn off or change the pricing or change the quality of or sensor tomorrow right well okay on the censoring point this thing is pretty damn censored already but at least you know what you're getting for me personally the number one thing I see this being used for is chatbot thoughts that belong to companies and this is going to be the go-to model moving forward no more openai API with rate limits and having to explain to the clients that there's a few dollars of extra cost depending on the usage no you just download this thing build this into the bot and you have a self-contained version where the data is not being sent around to open Ai and back it's all right there the model the chat interface it's all yours and you don't know anybody anything and that's the big difference here because in this ballsy move by meta and Microsoft they really change the game and forced a lot of other players to act more openly because they just set the standard for what a good model is supposed to look like and what the licensing around it is supposed to look like this is really a fantastic Direction they're pushing the space into and I hope that this video helped you understand what is actually happening here because it's a big deal if you take everything we just talked about and you combine it with what I covered in this video you'll realize that soon we'll get to take open source models train them with people's personalities and you'll have like an embodied person inside of the language model it's absolutely crazy but check out this video to understand what I'm talking about because there I uncover a hidden capability with franchise apt where you can start talking to certain people just based on their Wikipedia page crazy times we live in but better get used to it```\", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "url = urls[1]\n",
    "questions = \"How does LLAMA 2 perform compared to ChatGPT?\"\n",
    "\n",
    "summary = summarize_from_url(url, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LLAMA 2 is an open source language model that is claimed to be better than GPT 3.5. It is more up to date and can be used for research and commercial purposes. LLAMA 2 has been heavily fine-tuned to align with human preferences, making it safer and more usable. It comes in three variations with different parameter sizes. LLAMA 2 performs better than ChatGPT in terms of safety and benchmark tests. It is considered to be the safest large language model and performs well in various academic benchmarks. LLAMA 2 is free to use and can be downloaded for building applications. It is suitable for chatbot applications and provides a self-contained version without relying on external language models. Overall, LLAMA 2 is a significant development in the AI space, pushing for more open models and setting a standard for licensing and model quality.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of the Basic Use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
